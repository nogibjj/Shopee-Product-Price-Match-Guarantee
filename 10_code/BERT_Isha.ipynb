{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_path = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'\n",
    "training_dataset =pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')\n",
    "testing_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash   \n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433  \\\n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash   \n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a  \\\n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# remove numbers\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# remove extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def word_tokenize(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
    "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
    "    return filtered_sentence\n",
    "\n",
    "# remove all preprocessing\n",
    "def remove_all_preprocessing(text):\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text\n",
    "\n",
    "# apply all preprocessing\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "testing_dataset['title'] = testing_dataset['title'].apply(lambda x: remove_all_preprocessing(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape M VHB mm x ORIGINAL DOUBLE FOAM TAPE</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek Motif Acak Campur L...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc\\xclair Latte ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven gsm ply l...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft S Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RES mm Subwoofer Bass Metal Wi...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung Sap...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR ISOLASI AJAIB ANTI BOCOR</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image   \n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  \\\n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title   \n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret  \\\n",
       "1      af3f9460c2838f0f   Double Tape M VHB mm x ORIGINAL DOUBLE FOAM TAPE   \n",
       "2      b94cb00ed3e50f78            Maling TTS Canned Pork Luncheon Meat gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek Motif Acak Campur L...   \n",
       "4      a6f319f924ad708c                        Nescafe \\xc\\xclair Latte ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven gsm ply l...   \n",
       "34246  be86851f72e2853c           MamyPoko Pants Royal Soft S Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RES mm Subwoofer Bass Metal Wi...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung Sap...   \n",
       "34249  af8bc4b2d2cf9083   FLEX TAPE PELAPIS BOCOR ISOLASI AJAIB ANTI BOCOR   \n",
       "\n",
       "       label_group  \n",
       "0        249114794  \n",
       "1       2937985045  \n",
       "2       2395904891  \n",
       "3       4093212188  \n",
       "4       3648931069  \n",
       "...            ...  \n",
       "34245   3776555725  \n",
       "34246   2736479533  \n",
       "34247   4101248785  \n",
       "34248   1663538013  \n",
       "34249    459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of \\\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.replace('\\\\', ''))\n",
    "# lower case\n",
    "training_dataset['title'] = training_dataset['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>double tape m vhb mm x original double foam tape</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>maling tts canned pork luncheon meat gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>daster batik lengan pendek motif acak campur l...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>nescafe xcxclair latte ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_2464356923</td>\n",
       "      <td>0013e7355ffc5ff8fb1ccad3e42d92fe.jpg</td>\n",
       "      <td>bbd097a7870f4a50</td>\n",
       "      <td>celana wanita bb kgharem wanita bisa cod</td>\n",
       "      <td>2660605217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_1802986387</td>\n",
       "      <td>00144a49c56599d45354a1c28104c039.jpg</td>\n",
       "      <td>f815c9bb833ab4c8</td>\n",
       "      <td>jubah anak size thn</td>\n",
       "      <td>1835033137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_1806152124</td>\n",
       "      <td>0014f61389cbaa687a58e38a97b6383d.jpg</td>\n",
       "      <td>eea7e1c0c04da33d</td>\n",
       "      <td>kulot plisket salur candy plisket wish kulot p...</td>\n",
       "      <td>1565741687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_86570404</td>\n",
       "      <td>0019a3c6755a194cb2e2c12bfc63972e.jpg</td>\n",
       "      <td>ea9af4f483249972</td>\n",
       "      <td>[logu] tempelan kulkas magnet angka tempelan a...</td>\n",
       "      <td>2359912463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_831680791</td>\n",
       "      <td>001be52b2beec40ddc1d2d7fc7a68f08.jpg</td>\n",
       "      <td>e1ce953d1a70618f</td>\n",
       "      <td>big sale sepatu pantofel kulit keren kerja kan...</td>\n",
       "      <td>2630990665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_1598329973</td>\n",
       "      <td>001d7f5d9a2fac714f4d5f37b3baffb4.jpg</td>\n",
       "      <td>bec8d09693634b4b</td>\n",
       "      <td>atasan rajut wanita lisdia sweater</td>\n",
       "      <td>2462407944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_2496690777</td>\n",
       "      <td>001e11145b8e9bf5ac51110c0fdd8697.jpg</td>\n",
       "      <td>eab5c295966ac368</td>\n",
       "      <td>pashmina kusut rawis polos crinkle shawl murah...</td>\n",
       "      <td>509010932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_2771755203</td>\n",
       "      <td>001e11145b8e9bf5ac51110c0fdd8697.jpg</td>\n",
       "      <td>eab5c295966ac368</td>\n",
       "      <td>pashmina kusut rawis polos crinkle shawl murah...</td>\n",
       "      <td>509010932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_998568945</td>\n",
       "      <td>001f4c8331d0554d133b10d85b7fafb2.jpg</td>\n",
       "      <td>d8a6082bb93d2db5</td>\n",
       "      <td>lampu led speedometer dashboard motor mobil sp...</td>\n",
       "      <td>4206465630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_4287573913</td>\n",
       "      <td>001f5580b058c6b8e33132190a757318.jpg</td>\n",
       "      <td>dc85e1750687f932</td>\n",
       "      <td>charger vizz vztc batok charger vizz a origina...</td>\n",
       "      <td>1932232224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_4196427721</td>\n",
       "      <td>002039aaf8618627a0442d5e89e5dda6.jpg</td>\n",
       "      <td>e98c873acc65946e</td>\n",
       "      <td>korek kuping led untuk balita cherrybabykidssh...</td>\n",
       "      <td>349297863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_3009013664</td>\n",
       "      <td>0027aaf8dd8bdbf0e4f2c19024e436cf.jpg</td>\n",
       "      <td>b3cccc26cc3333cc</td>\n",
       "      <td>marks spencer rose hand body lotion ml</td>\n",
       "      <td>1574620312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_3054371232</td>\n",
       "      <td>00286d2760e433a8a01cbd9e056144f7.jpg</td>\n",
       "      <td>be85837b8ca44ad3</td>\n",
       "      <td>saffron gr sd gr super negin premium quality o...</td>\n",
       "      <td>2353399596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_2985955659</td>\n",
       "      <td>002f978c58a44a00aadfca71c3cad2bb.jpg</td>\n",
       "      <td>bf38f0e083d7c710</td>\n",
       "      <td>hnkfashion sweater hoodie who printing babyter...</td>\n",
       "      <td>3415582503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_2961381387</td>\n",
       "      <td>00303ad1c062fdeaf5f41b9ffb71a5fb.jpg</td>\n",
       "      <td>e48d9b652098efe1</td>\n",
       "      <td>madame gie makeup blush on by gisell</td>\n",
       "      <td>2098400894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          posting_id                                 image       image_phash   \n",
       "0    train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433  \\\n",
       "1   train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2   train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3   train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4   train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "5   train_2464356923  0013e7355ffc5ff8fb1ccad3e42d92fe.jpg  bbd097a7870f4a50   \n",
       "6   train_1802986387  00144a49c56599d45354a1c28104c039.jpg  f815c9bb833ab4c8   \n",
       "7   train_1806152124  0014f61389cbaa687a58e38a97b6383d.jpg  eea7e1c0c04da33d   \n",
       "8     train_86570404  0019a3c6755a194cb2e2c12bfc63972e.jpg  ea9af4f483249972   \n",
       "9    train_831680791  001be52b2beec40ddc1d2d7fc7a68f08.jpg  e1ce953d1a70618f   \n",
       "10  train_1598329973  001d7f5d9a2fac714f4d5f37b3baffb4.jpg  bec8d09693634b4b   \n",
       "11  train_2496690777  001e11145b8e9bf5ac51110c0fdd8697.jpg  eab5c295966ac368   \n",
       "12  train_2771755203  001e11145b8e9bf5ac51110c0fdd8697.jpg  eab5c295966ac368   \n",
       "13   train_998568945  001f4c8331d0554d133b10d85b7fafb2.jpg  d8a6082bb93d2db5   \n",
       "14  train_4287573913  001f5580b058c6b8e33132190a757318.jpg  dc85e1750687f932   \n",
       "15  train_4196427721  002039aaf8618627a0442d5e89e5dda6.jpg  e98c873acc65946e   \n",
       "16  train_3009013664  0027aaf8dd8bdbf0e4f2c19024e436cf.jpg  b3cccc26cc3333cc   \n",
       "17  train_3054371232  00286d2760e433a8a01cbd9e056144f7.jpg  be85837b8ca44ad3   \n",
       "18  train_2985955659  002f978c58a44a00aadfca71c3cad2bb.jpg  bf38f0e083d7c710   \n",
       "19  train_2961381387  00303ad1c062fdeaf5f41b9ffb71a5fb.jpg  e48d9b652098efe1   \n",
       "\n",
       "                                                title  label_group  \n",
       "0                           paper bag victoria secret    249114794  \n",
       "1    double tape m vhb mm x original double foam tape   2937985045  \n",
       "2             maling tts canned pork luncheon meat gr   2395904891  \n",
       "3   daster batik lengan pendek motif acak campur l...   4093212188  \n",
       "4                           nescafe xcxclair latte ml   3648931069  \n",
       "5            celana wanita bb kgharem wanita bisa cod   2660605217  \n",
       "6                                 jubah anak size thn   1835033137  \n",
       "7   kulot plisket salur candy plisket wish kulot p...   1565741687  \n",
       "8   [logu] tempelan kulkas magnet angka tempelan a...   2359912463  \n",
       "9   big sale sepatu pantofel kulit keren kerja kan...   2630990665  \n",
       "10                 atasan rajut wanita lisdia sweater   2462407944  \n",
       "11  pashmina kusut rawis polos crinkle shawl murah...    509010932  \n",
       "12  pashmina kusut rawis polos crinkle shawl murah...    509010932  \n",
       "13  lampu led speedometer dashboard motor mobil sp...   4206465630  \n",
       "14  charger vizz vztc batok charger vizz a origina...   1932232224  \n",
       "15  korek kuping led untuk balita cherrybabykidssh...    349297863  \n",
       "16             marks spencer rose hand body lotion ml   1574620312  \n",
       "17  saffron gr sd gr super negin premium quality o...   2353399596  \n",
       "18  hnkfashion sweater hoodie who printing babyter...   3415582503  \n",
       "19               madame gie makeup blush on by gisell   2098400894  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['title_len'] = training_dataset['title'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=training_dataset, x='title_len', bins=30)\n",
    "# add title to plot\n",
    "# add axis titles \n",
    "plt.xlabel('Title length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of title length')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>double tape m vhb mm x original double foam tape</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>maling tts canned pork luncheon meat gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>daster batik lengan pendek motif acak campur l...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>nescafe xcxclair latte ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>masker bahan kain spunbond non woven gsm ply l...</td>\n",
       "      <td>3776555725</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>mamypoko pants royal soft s popok celana</td>\n",
       "      <td>2736479533</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>khanzaacc robot res mm subwoofer bass metal wi...</td>\n",
       "      <td>4101248785</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>kaldu non msg halal mama kamu ayam kampung sap...</td>\n",
       "      <td>1663538013</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>flex tape pelapis bocor isolasi ajaib anti bocor</td>\n",
       "      <td>459464107</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image   \n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  \\\n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title   \n",
       "0      94974f937d4c2433                          paper bag victoria secret  \\\n",
       "1      af3f9460c2838f0f   double tape m vhb mm x original double foam tape   \n",
       "2      b94cb00ed3e50f78            maling tts canned pork luncheon meat gr   \n",
       "3      8514fc58eafea283  daster batik lengan pendek motif acak campur l...   \n",
       "4      a6f319f924ad708c                          nescafe xcxclair latte ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  masker bahan kain spunbond non woven gsm ply l...   \n",
       "34246  be86851f72e2853c           mamypoko pants royal soft s popok celana   \n",
       "34247  ad27f0d08c0fcbf0  khanzaacc robot res mm subwoofer bass metal wi...   \n",
       "34248  e3b13bd1d896c05c  kaldu non msg halal mama kamu ayam kampung sap...   \n",
       "34249  af8bc4b2d2cf9083   flex tape pelapis bocor isolasi ajaib anti bocor   \n",
       "\n",
       "       label_group  title_len  \n",
       "0        249114794         25  \n",
       "1       2937985045         48  \n",
       "2       2395904891         39  \n",
       "3       4093212188         84  \n",
       "4       3648931069         25  \n",
       "...            ...        ...  \n",
       "34245   3776555725         62  \n",
       "34246   2736479533         40  \n",
       "34247   4101248785         57  \n",
       "34248   1663538013         80  \n",
       "34249    459464107         48  \n",
       "\n",
       "[34250 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.11.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4745:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4745:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4745:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5233:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pytagcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(training_dataset['title'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = [(word, count) for word, count in word_counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_freqs.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pytagcloud.make_tags(word_freqs, maxsize=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytagcloud.create_tag_image(tags, 'word_cloud.png', size=(800, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vscode/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/vscode/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 287MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "img_encoder = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the model parameters\n",
    "for param in img_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last layer with an identity layer to get the image embeddings\n",
    "img_encoder.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)821d1/.gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 391/391 [00:00<00:00, 533kB/s]\n",
      "Downloading (â€¦)_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 200kB/s]\n",
      "Downloading (â€¦)8d01e821d1/README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.95k/3.95k [00:00<00:00, 4.37MB/s]\n",
      "Downloading (â€¦)d1/added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.00/2.00 [00:00<00:00, 2.38kB/s]\n",
      "Downloading (â€¦)01e821d1/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 625/625 [00:00<00:00, 747kB/s]\n",
      "Downloading (â€¦)ce_transformers.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [00:00<00:00, 160kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438M/438M [00:01<00:00, 322MB/s] \n",
      "Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.0/53.0 [00:00<00:00, 42.3kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 85.3kB/s]\n",
      "Downloading (â€¦)821d1/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 72.5MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 399/399 [00:00<00:00, 371kB/s]\n",
      "Downloading (â€¦)8d01e821d1/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 58.3MB/s]\n",
      "Downloading (â€¦)1e821d1/modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 229/229 [00:00<00:00, 227kB/s]\n"
     ]
    }
   ],
   "source": [
    "text_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)), # resize the images to 256x256\n",
    "        transforms.ToTensor(), # convert the images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], # normalize the images\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the images from the folder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[39m=\u001b[39m ImageFolder(root\u001b[39m=\u001b[39;49mdata_dir, transform\u001b[39m=\u001b[39;49mdata_transforms)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the images from the folder\n",
    "dataset = ImageFolder(root=data_dir, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39m'\u001b[39;49m\u001b[39m/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m img \u001b[39m=\u001b[39m transform(img)\n\u001b[1;32m      3\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# add batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3235\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3236\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3237\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3239\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images'"
     ]
    }
   ],
   "source": [
    "img = Image.open('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train_images')\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)  # add batch dimension\n",
    "img = img.to(device)  # move image to GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3137877218.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[35], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    img = # load and preprocess image\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "img_embeddings = []\n",
    "for img_path in df['image']:\n",
    "    img = # load and preprocess image\n",
    "    img_embedding = img_encoder.encode(img)\n",
    "    img_embeddings.append(img_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 743kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [00:01<00:00, 317MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pooling layer\n",
    "pooling_layer = nn.MaxPool1d(training_dataset.shape[0])\n",
    "\n",
    "# Define batch normalization layer\n",
    "batch_norm_layer = nn.BatchNorm1d(768)\n",
    "\n",
    "# Define linear layer\n",
    "linear_layer = nn.Linear(768, 1)\n",
    "\n",
    "# Define dropout layer\n",
    "dropout_layer = nn.Dropout(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(linear_layer.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image   \n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  \\\n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title   \n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret  \\\n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n",
       "34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n",
       "34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n",
       "\n",
       "       label_group  \n",
       "0        249114794  \n",
       "1       2937985045  \n",
       "2       2395904891  \n",
       "3       4093212188  \n",
       "4       3648931069  \n",
       "...            ...  \n",
       "34245   3776555725  \n",
       "34246   2736479533  \n",
       "34247   4101248785  \n",
       "34248   1663538013  \n",
       "34249    459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "maximum_length = 128\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/vscode/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(training_dataset)):\n",
    "    # Tokenize input text\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        training_dataset['title'][i],                      # Title to encode\n",
    "                        add_special_tokens = True,                  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = maximum_length,                        # Pad or truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        #return_attention_mask = True,               # Construct attn. masks\n",
    "                        return_tensors = 'pt',                      # Return PyTorch tensors\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.append(encoded_dict['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And its attention mask (simply differentiates padding from non-padding).\n",
    "attention_masks.append(encoded_dict['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Add the label to the list so that we can convert it to a tensor later.\n",
    "labels.append(training_dataset['label_group'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pooling layer\n",
    "pooling_layer = nn.MaxPool1d(input_ids.shape[0])\n",
    "\n",
    "# Define batch normalization layer\n",
    "batch_norm_layer = nn.BatchNorm1d(768)\n",
    "\n",
    "# Define linear layer\n",
    "linear_layer = nn.Linear(768, 1)\n",
    "\n",
    "# Define dropout layer\n",
    "dropout_layer = nn.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(linear_layer.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1 elements not 768",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m pooled_output \u001b[39m=\u001b[39m dropout_layer(pooled_output)\n\u001b[1;32m     12\u001b[0m pooled_output \u001b[39m=\u001b[39m pooled_output\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# add a dimension for batch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pooled_output \u001b[39m=\u001b[39m batch_norm_layer(pooled_output)\n\u001b[1;32m     14\u001b[0m pooled_output \u001b[39m=\u001b[39m pooling_layer(pooled_output)\n\u001b[1;32m     15\u001b[0m pooled_output \u001b[39m=\u001b[39m pooled_output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 1 elements not 768"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(training_dataset)):\n",
    "        # Get input data\n",
    "        input_id = input_ids[i].unsqueeze(0)\n",
    "        attention_mask = attention_masks[i].unsqueeze(0)\n",
    "        label = labels[i].unsqueeze(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = bert_model(input_ids=input_id, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = dropout_layer(pooled_output)\n",
    "        pooled_output = pooled_output.unsqueeze(0)  # add a dimension for batch\n",
    "        pooled_output = batch_norm_layer(pooled_output)\n",
    "        pooled_output = pooling_layer(pooled_output)\n",
    "        pooled_output = pooled_output.squeeze(1)\n",
    "        logits = linear_layer(pooled_output)\n",
    "        loss = loss_function(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "train_embeddings = model.encode(training_dataset['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pooling layer\n",
    "pooling_layer = nn.MaxPool1d(training_dataset.shape[0])\n",
    "\n",
    "# Define batch normalization layer\n",
    "batch_norm_layer = nn.BatchNorm1d(768)\n",
    "\n",
    "# Define linear layer\n",
    "linear_layer = nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout layer\n",
    "dropout_layer = nn.Dropout(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(linear_layer.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "number_of_top_scores_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/vscode/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "for title in training_dataset['title']:\n",
    "    encoded_dict = tokenizer.encode_plus(title,\n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=64,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert_model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Define BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain() \u001b[39m# set model to training mode\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      5\u001b[0m     input_ids, attention_masks, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train() # set model to training mode\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        input_ids, attention_masks, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = bert_model(input_ids=input_ids,\n",
    "                             attention_mask=attention_masks)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = torch.nn.functional.dropout(pooled_output, p=0.2)\n",
    "        pooled_output = torch.nn.functional.batch_norm(pooled_output)\n",
    "        pooled_output = torch.nn.functional.avg_pool1d(pooled_output.unsqueeze(1), kernel_size=64).squeeze(1)\n",
    "        logits = torch.nn.Linear(768, 2)(pooled_output)\n",
    "        \n",
    "        # Compute loss and backward pass\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Validation loop\n",
    "    model.eval() # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            input_ids, attention_masks, labels = data\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = bert_model(input_ids=input_ids,\n",
    "                                 attention_mask=attention_masks)\n",
    "            pooled_output = outputs[1]\n",
    "            pooled_output = torch.nn.functional.dropout(pooled_output, p=0.2)\n",
    "            pooled_output = torch.nn.functional.batch_norm(pooled_output)\n",
    "            pooled_output = torch.nn.functional.avg_pool1d(pooled_output.unsqueeze(1), kernel_size=64).squeeze(1)\n",
    "            logits = torch.nn.Linear(768, 2)(pooled_output)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            val_loss = loss_function(logits, labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    # Print stats for this epoch\n",
    "    print('Epoch [%d], Loss: %.4f, Val Loss: %.4f, Accuracy: %.2f%%' %\n",
    "          (epoch + 1, running_loss / len(train_dataloader), val_running_loss / len(val_dataloader), 100 * correct / total))\n",
    "          \n",
    "    model.train() # set model back to training mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-13 20:42:20.001788: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-13 20:42:22.580692: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-13 20:42:22.581622: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 20:42:27.176850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230k/230k [00:00<00:00, 78.2MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 122kB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62.0/62.0 [00:00<00:00, 85.0kB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 468/468 [00:00<00:00, 614kB/s]\n",
      "Downloading tf_model.h5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 545M/545M [00:16<00:00, 32.9MB/s] \n",
      "2023-04-13 20:42:49.012780: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m TFBertModel\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 8\u001b[0m bert_title_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((training_dataset\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m768\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m idx,txt \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(training_dataset[\u001b[39m'\u001b[39m\u001b[39mprep_title\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m     11\u001b[0m   encoded_input \u001b[39m=\u001b[39m tokenizer(txt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input data\n",
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/vscode/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for title in training_dataset['title']:\n",
    "    encoded_dict = tokenizer.encode_plus(title,\n",
    "                                         add_special_tokens=True,\n",
    "                                         max_length=64,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_group\n",
       "994676122     51\n",
       "1163569239    51\n",
       "1141798720    51\n",
       "159351600     51\n",
       "562358068     51\n",
       "              ..\n",
       "1349674444     2\n",
       "2359590024     2\n",
       "1528129430     2\n",
       "3156068330     2\n",
       "53836859       2\n",
       "Name: count, Length: 11014, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training dataset columns\n",
    "# posting_id, image, image_phash, title, label_group, matches\n",
    "\n",
    "# show label_group \n",
    "training_dataset['label_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_labels \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(labels))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "num_labels = len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m pooled_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mbatch_norm(pooled_output, running_mean\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, running_var\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m pooled_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mavg_pool1d(pooled_output\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), kernel_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(\u001b[39m768\u001b[39m, num_labels)(pooled_output)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Compute loss and backward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m loss_function(logits, torch\u001b[39m.\u001b[39mzeros(batch_size, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_labels' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        input_ids, attention_masks = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_masks)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = torch.nn.functional.dropout(pooled_output, p=0.2)\n",
    "        pooled_output = torch.nn.functional.batch_norm(pooled_output, running_mean=None, running_var=None, training=True)\n",
    "        pooled_output = torch.nn.functional.avg_pool1d(pooled_output.unsqueeze(1), kernel_size=64).squeeze(1)\n",
    "        logits = torch.nn.Linear(768, num_labels)(pooled_output)\n",
    "\n",
    "        # Compute loss and backward pass\n",
    "        loss = loss_function(logits, torch.zeros(batch_size, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print running loss every 100 batches\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 512\n",
    "output_features = 128\n",
    "\n",
    "# Define the dropout probability and batch normalization momentum\n",
    "dropout_prob = 0.2\n",
    "batch_norm_momentum = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPoolModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTPoolModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_tokens, momentum=batch_norm_momentum)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(output_features)\n",
    "        self.linear = nn.Linear(output_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = self.batch_norm(pooled_output)\n",
    "        pooled_output = pooled_output.unsqueeze(1)\n",
    "        pooled_output = self.pooling(pooled_output)\n",
    "        pooled_output = pooled_output.squeeze(1)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device to use for training the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model and send it to the device\n",
    "model = BERTPoolModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv('/workspaces/Shopee-Price-Match-Guarantee/00_source_data/shopee-product-matching/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of epochs\n",
    "batch_size = 16\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/vscode/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert the titles to input IDs and attention masks\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for title in training_dataset['title']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        title,                      # Title to encode\n",
    "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,            # Pad/truncate to 64 tokens\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,# Generate attention mask\n",
    "                        return_tensors = 'pt',      # PyTorch tensors format\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.zeros(len(input_ids), dtype=torch.long)  # No labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert_model.parameters(), lr=2e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(training_dataset):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for title in training_dataset['title']:\n",
    "        encoded = tokenizer.encode_plus(title, add_special_tokens=True, max_length=64, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    dataset = torch.utils.data.TensorDataset(input_ids, attention_masks)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "33375",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 33375",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[39m# Get the inputs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         titles, _ \u001b[39m=\u001b[39m data\n\u001b[1;32m      6\u001b[0m         titles \u001b[39m=\u001b[39m titles\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 33375"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Get the inputs\n",
    "        titles, _ = data\n",
    "        titles = titles.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        bert_output = bert_model(titles)\n",
    "        pooled_output = bert_output[1]\n",
    "        logits = torch.nn.Linear(768, 1)(pooled_output)\n",
    "        \n",
    "        # Compute loss and backward pass\n",
    "        loss = loss_function(logits, torch.zeros(batch_size, dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print running loss every 100 batches\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['posting_id', 'image', 'image_phash', 'title', 'label_group'], dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names of training dataset\n",
    "training_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 249114794 2937985045 2395904891 ... 1313560418  763032672   53836859]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "unique_labels = training_dataset['label_group'].unique()\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(self.bert_model.config.hidden_size)\n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = torch.nn.Linear(self.bert_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "        x = self.dropout(bert_output)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.avg_pool(x).squeeze(-1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "15199",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 15199",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[39m# Get the inputs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         input_ids, attention_masks, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m      6\u001b[0m         input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 15199"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Get the inputs\n",
    "        input_ids, attention_masks, labels = data\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        bert_output = bert_model(input_ids, attention_mask=attention_masks)\n",
    "        pooled_output = bert_output[1]\n",
    "        pooled_output = torch.nn.functional.dropout(pooled_output, p=0.2)\n",
    "        pooled_output = torch.nn.BatchNorm1d(pooled_output.shape[1])(pooled_output)\n",
    "        pooled_output = torch.nn.functional.avg_pool1d(pooled_output.unsqueeze(1), kernel_size=64).squeeze(1)\n",
    "        logits = torch.nn.Linear(768, 1)(pooled_output)\n",
    "        \n",
    "        # Compute loss and backward pass\n",
    "        loss = loss_function(logits, labels.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print running loss every 100 batches\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image   \n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  \\\n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title   \n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret  \\\n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n",
       "34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n",
       "34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n",
       "\n",
       "       label_group  \n",
       "0        249114794  \n",
       "1       2937985045  \n",
       "2       2395904891  \n",
       "3       4093212188  \n",
       "4       3648931069  \n",
       "...            ...  \n",
       "34245   3776555725  \n",
       "34246   2736479533  \n",
       "34247   4101248785  \n",
       "34248   1663538013  \n",
       "34249    459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for title in training_dataset['title']:\n",
    "    input_ids = torch.tensor(tokenizer.encode(title, add_special_tokens=True, max_length=64, padding='max_length')).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]\n",
    "    embeddings.append(last_hidden_states[0][0].numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the embeddings for each product title\n",
    "embeddings = []\n",
    "for title in shopee_df['title']:\n",
    "    tokens = tokenizer.encode(title, add_special_tokens=True)\n",
    "    tensor = torch.tensor([tokens])\n",
    "    outputs, _ = model(tensor)\n",
    "    embedding = outputs.detach().numpy()[0]\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Compute the similarity between two product titles\n",
    "index_1 = 0  # index of the first product title\n",
    "index_2 = 1  # index of the second product title\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(torch.tensor([embeddings[index_1]]), torch.tensor([embeddings[index_2]]))\n",
    "\n",
    "print(cosine_sim.item())  # Output: similarity between the two product titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame.from_dict(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/distiluse-base-multilingual-cased-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_top_scores_k = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)6015c/.gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:00<00:00, 1.11MB/s]\n",
      "Downloading (â€¦)_Pooling/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 180kB/s]\n",
      "Downloading (â€¦)/2_Dense/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:00<00:00, 182kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.58M/1.58M [00:00<00:00, 130MB/s]\n",
      "Downloading (â€¦)ff6066015c/README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.38k/2.38k [00:00<00:00, 2.86MB/s]\n",
      "Downloading (â€¦)6066015c/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 610/610 [00:00<00:00, 963kB/s]\n",
      "Downloading (â€¦)ce_transformers.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122/122 [00:00<00:00, 93.9kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 539M/539M [00:01<00:00, 314MB/s] \n",
      "Downloading (â€¦)nce_bert_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.0/53.0 [00:00<00:00, 66.3kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 114kB/s]\n",
      "Downloading (â€¦)6015c/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.96M/1.96M [00:00<00:00, 84.6MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 531/531 [00:00<00:00, 839kB/s]\n",
      "Downloading (â€¦)ff6066015c/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 996k/996k [00:00<00:00, 91.3MB/s]\n",
      "Downloading (â€¦)066015c/modules.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [00:00<00:00, 409kB/s]\n"
     ]
    }
   ],
   "source": [
    "bert_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer = nn.AdaptiveMaxPool1d(1)\n",
    "batch_norm_layer = nn.BatchNorm1d(bert_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass function\n",
    "def forward_pass(input_ids, attention_mask):\n",
    "    embeddings = bert_model(input_ids, attention_mask)\n",
    "    embeddings = embeddings.unsqueeze(-1)\n",
    "    embeddings = pooling_layer(embeddings)\n",
    "    embeddings = embeddings.squeeze(-1)\n",
    "    embeddings = batch_norm_layer(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(title, \u001b[39mstr\u001b[39m):\n\u001b[1;32m      5\u001b[0m     title \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(title)\n\u001b[0;32m----> 6\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(bert_model\u001b[39m.\u001b[39;49mtokenize(title))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(input_ids)\n\u001b[1;32m      8\u001b[0m embeddings \u001b[39m=\u001b[39m forward_pass(input_ids, attention_mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "train_embeddings = []\n",
    "for i in range(len(training_dataset)):\n",
    "    title = training_dataset['title'][i]\n",
    "    if not isinstance(title, str):\n",
    "        title = str(title)\n",
    "    input_ids = torch.tensor(bert_model.tokenize(title)).unsqueeze(0)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    embeddings = forward_pass(input_ids, attention_mask)\n",
    "    train_embeddings.append(embeddings.detach().numpy())\n",
    "\n",
    "train_embeddings = np.vstack(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>title_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>double tape m vhb mm x original double foam tape</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>maling tts canned pork luncheon meat gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>daster batik lengan pendek motif acak campur l...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>nescafe xcxclair latte ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash   \n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433  \\\n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  title_len  \n",
       "0                          paper bag victoria secret    249114794         25  \n",
       "1   double tape m vhb mm x original double foam tape   2937985045         48  \n",
       "2            maling tts canned pork luncheon meat gr   2395904891         39  \n",
       "3  daster batik lengan pendek motif acak campur l...   4093212188         84  \n",
       "4                          nescafe xcxclair latte ml   3648931069         25  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable as one-hot vectors\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns of training_dataset \n",
    "\n",
    "\n",
    "# Encode the target variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
